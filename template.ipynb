{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xpand IT DS Challenge\n",
    "\n",
    "This notebook contains the template you should use to present your code, results and conclusions. You should keep the main structure intact to make it easier to evaluate and compare in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "You can find the dataset in the data folder of the repository. The folder contains two files:\n",
    "* dow_jones_index.data - dataset data\n",
    "* dow_jones_index.names - dataset information and details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Analysis\n",
    "Here you should conduct a brief analysis of what is Dow Jones Index. You can enumerate the main topics to take into account based on the dataset provided as well as your understandings of the variables.\n",
    "\n",
    "\n",
    "-----\n",
    "*Add here your business analysis conclusions (max. 200 words)*\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "During the data understanding phase, you should focus on understanding what each variable represents, compute statistics and visualizations. Some questions that may guide your work follow:\n",
    "* Feature engineering: should new features be created from the existing ones?\n",
    "* What will be your features and your label?\n",
    "* Is the dataset ready for the prediction task? (ex: missing values)\n",
    "* How will the data be split into train and test sets?\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.figure as fgr\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, mean_squared_error, root_mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dow_jones_index.data\", sep=',')\n",
    "print(df.shape)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12012/1739838972.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Check data types and missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Check data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return number of missing values for each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert stock to category and date to datetime\n",
    "df['stock'] = df['stock'].astype('category')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#Select all the remeining object columns (which are actually currency columns)\n",
    "currency_features = df.select_dtypes(include=['object']).columns\n",
    "#Remove the dollar sign from currency columns and convert to float\n",
    "df[currency_features] = df[currency_features].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "#Check the types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics for the dataset\n",
    "df.describe().T.apply(lambda s: s.apply('{0:.3f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot the correlation matrix heatmap of all numerical variables in data\n",
    "def correlation_matrix(df):\n",
    "    columns_of_interest = df.select_dtypes(include=[np.number]).columns\n",
    "    df_selected = df[columns_of_interest]\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df_selected.corr(method = 'pearson')\n",
    "\n",
    "    # Configure the size of the figure\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Correlation heatmap\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f')\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create violin plots of all numerical variables in data, checking distribution and density\n",
    "def plot_violins(data):\n",
    "    columns_of_interest = data.select_dtypes(include=[np.number]).columns\n",
    "    plt.figure(figsize=(18, 18))\n",
    "\n",
    "    # Loop to create boxplots for each numeric column\n",
    "    for i, column in enumerate(columns_of_interest, 1):\n",
    "        plt.subplot(4, 4, i)\n",
    "        sns.violinplot(data=data, y=column, palette='viridis')\n",
    "        plt.title(f'Boxplot of {column}')\n",
    "        plt.ylabel(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    \n",
    "plot_violins(df)\n",
    "\n",
    "#Line plot of stock closing prices over time\n",
    "def line_plot_feature(data, column):\n",
    "    sns.set_theme(rc={'figure.figsize':(15, 10)})\n",
    "    lineplot = sns.lineplot(data=data, x='date', y= column, hue='stock')\n",
    "    plt.title('Stock Closing Prices Over Time')\n",
    "    plt.show()\n",
    "\n",
    "line_plot_feature(df, 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_plot_feature(df, 'percent_change_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = df.select_dtypes(include=[np.number]).columns\n",
    "sns.pairplot(df[columns_of_interest])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the viridis colormap\n",
    "viridis_palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Create a pair plot with the viridis colormap\n",
    "pair_plot = sns.pairplot(df[['percent_change_next_weeks_price', 'percent_change_price', 'previous_weeks_volume']], \n",
    "                         diag_kind='kde', \n",
    "                         markers='o',\n",
    "                         palette=viridis_palette)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate volatility (high-low spread)\n",
    "df['volatility'] = df['high'] - df['low']\n",
    "\n",
    "#Rolling averages and volatility over 3 weeks\n",
    "df['rolling_avg_3w'] = df.groupby('stock')['close'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "df['rolling_vol_3w'] = df.groupby('stock')['volatility'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "\n",
    "#df['ma_7'] = df.groupby('stock')['close'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "#df['ma_3'] = df.groupby('stock')['close'].rolling(window=3).mean().reset_index(0, drop=True)\n",
    "\n",
    "#Lag features of previous week's percent change\n",
    "df['lag_1'] = df.groupby('stock')['percent_change_price'].shift(1)\n",
    "df['lag_2'] = df.groupby('stock')['percent_change_price'].shift(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "*Add here your data understanding findings and conclusions (max. 200 words)*\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "In this phase, your main goal is to develop and describe your approach to the solution of the problem. Some guidelines to help you:\n",
    "* What metrics will you use to evaluate your solutions?\n",
    "* What are some algorithms that can lead to good results? And why?\n",
    "* Describe in detail your thought process during the development of your solution.\n",
    "* Present your results.\n",
    "\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create target column: 1 if it's the best-performing stock for that week, otherwise 0\n",
    "df['best_stock'] = df.groupby('date')['percent_change_next_weeks_price'].transform(lambda x: (x == x.max()).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define features and target\n",
    "features_with_date = ['date', 'stock', 'open', 'high', 'low', 'close', 'volume', 'percent_change_price', \n",
    "            'percent_change_volume_over_last_wk', 'previous_weeks_volume', \n",
    "            'days_to_next_dividend', 'percent_return_next_dividend', 'volatility', 'rolling_avg_3w', 'rolling_vol_3w', 'lag_1', 'lag_2']\n",
    "\n",
    "\n",
    "features = ['open', 'high', 'low', 'close', 'volume', 'percent_change_price', \n",
    "            'percent_change_volume_over_last_wk', 'previous_weeks_volume', \n",
    "            'days_to_next_dividend', 'percent_return_next_dividend', 'volatility', 'rolling_avg_3w', 'rolling_vol_3w', 'lag_1', 'lag_2']\n",
    "\n",
    "target = 'best_stock'\n",
    "\n",
    "#Create a copy of dataframe and drop missing values\n",
    "df_xgb = df.copy()\n",
    "df_xgb = df_xgb.dropna()\n",
    "\n",
    "X = df_xgb[features_with_date]\n",
    "y = df_xgb[target]\n",
    "\n",
    "# Train-test split - first 70% for training, last 20% for testing\n",
    "train_size = int(len(df_xgb) * 0.7)\n",
    "train_data = df_xgb.iloc[:train_size]\n",
    "test_data = df_xgb.iloc[train_size:]\n",
    "\n",
    "X_train = train_data[features_with_date]\n",
    "y_train = train_data[target]\n",
    "X_test = test_data[features_with_date]\n",
    "y_test = test_data[target]\n",
    "\n",
    "#smote = SMOTE(random_state=2)\n",
    "#X_train_smote, y_train_smote = smote.fit_resample(X_train[features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train XGBoost model\n",
    "model = XGBClassifier(scale_pos_weight = 1.5, use_label_encoder=False)\n",
    "model.fit(X_train[features], y_train)\n",
    "\n",
    "#Evaluate performance\n",
    "y_pred = model.predict(X_test[features])\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy, precision, recall\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "data_test = X_test.copy()  # Copy test data\n",
    "data_test['predicted_best_stock'] = y_pred\n",
    "\n",
    "#Merge with the original data to get the actual returns for each predicted stock\n",
    "data_test = data_test.merge(df[['date', 'stock', 'percent_change_next_weeks_price']], on=['date','stock'], how='left')\n",
    "\n",
    "#Assuming 100€ investment each week\n",
    "data_test['predicted_return'] = data_test['percent_change_next_weeks_price']\n",
    "cumulative_return = data_test[data_test['predicted_best_stock']==1]['predicted_return'].sum()\n",
    "print(f'Cumulative Return: {cumulative_return} EUR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Forecasting with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of dataframe then sort and drop missing values\n",
    "df_lstm = df.copy()\n",
    "df_lstm.set_index('date', inplace=True)\n",
    "df_lstm.sort_index(axis=0,inplace=True)\n",
    "\n",
    "features = ['close','volume','volatility','rolling_vol_3w','rolling_avg_3w','percent_change_price','previous_weeks_volume','percent_return_next_dividend','percent_change_next_weeks_price']\n",
    "target = 'percent_change_next_weeks_price'\n",
    "\n",
    "df_lstm = df_lstm[features]\n",
    "df_lstm.fillna(0, inplace=True)\n",
    "\n",
    "#Preprocessing for LSTM\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df_lstm)\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length][:-1]) # Use seq_length data points as input (exclude the target)\n",
    "        y.append(data[i+seq_length][-1]) #Target is 'percent_change_next_weeks_price' (the last column)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 4  #use the last 4 weeks to predict the next week\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(0.7 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "#Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=120, batch_size=1, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert the scaling of the predicted data (back to original scale)\n",
    "y_pred_rescaled = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], y_pred), axis=1))[:, -1]\n",
    "y_test_rescaled = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
    "\n",
    "#Plot the predicted vs actual values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_rescaled, color='blue', label='Actual percent_change_next_weeks_price')\n",
    "plt.plot(y_pred_rescaled, color='red', label='Predicted percent_change_next_weeks_price')\n",
    "plt.title('LSTM Model - Predicted vs Actual Percent Change Next Week\\'s Price')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Percent Change Next Week\\'s Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "rmse = mean_squared_error(y_test_rescaled, y_pred_rescaled, squared=False)\n",
    "print(f'Root Mean Squared Error (MSE): {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_returns(data):\n",
    "\n",
    "    data['best_actual_stock'] = data.groupby('date')['percent_change_next_weeks_price'].transform(lambda x: (x == x.max()).astype(int))\n",
    "    data['best_predicted_stock'] = data.groupby('date')['predicted_change_next_weeks_price'].transform(lambda x: (x == x.max()).astype(int))\n",
    "\n",
    "    actual_returns_add = data[data['best_actual_stock']==1]['percent_change_next_weeks_price'].sum()\n",
    "    predicted_returns_add = data[data['best_predicted_stock']==1]['percent_change_next_weeks_price'].sum()\n",
    "\n",
    "    actual_returns_array = data[data['best_actual_stock']==1]['percent_change_next_weeks_price'].to_numpy()\n",
    "    predicted_returns_array = data[data['best_predicted_stock']==1]['percent_change_next_weeks_price'].to_numpy()\n",
    "\n",
    "    # Cumulative returns for both actual and predicted values (assuming initial investment of 100€)\n",
    "    initial_investment = 100\n",
    "    cumulative_actual_return = np.cumsum(actual_returns_array) * initial_investment / 100 + initial_investment\n",
    "    cumulative_predicted_return = np.cumsum(predicted_returns_array) * initial_investment / 100 + initial_investment\n",
    "\n",
    "    # Plot cumulative returns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cumulative_actual_return, label='Actual Cumulative Return', color='blue')\n",
    "    plt.plot(cumulative_predicted_return, label='Predicted Cumulative Return', color='red')\n",
    "    plt.title('Cumulative Returns (Actual vs Predicted)')\n",
    "    plt.xlabel('Weeks')\n",
    "    plt.ylabel('Cumulative Return (€)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Step 7: Evaluate model with Mean Squared Error (MSE) as an additional metric\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(actual_returns_array, predicted_returns_array)\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    rmse = root_mean_squared_error(actual_returns_array, predicted_returns_array)\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "    # Final cumulative returns\n",
    "    final_actual_return = cumulative_actual_return[-1]\n",
    "    final_predicted_return = cumulative_predicted_return[-1]\n",
    "\n",
    "    print(f\"Final Actual Cumulative Return: {final_actual_return:.2f} EUR\")\n",
    "    print(f\"Final Predicted Cumulative Return: {final_predicted_return:.2f} EUR\")\n",
    "    print(f\"Percentage of MAX returns acheived: {(100*predicted_returns_add)/actual_returns_add:.2f} %\")\n",
    "\n",
    "    print(cumulative_actual_return)\n",
    "    print(cumulative_predicted_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_test = df_lstm[train_size:]\n",
    "df_lstm_test = df_lstm_test[seq_length:]\n",
    "\n",
    "df_lstm_test['predicted_change_next_weeks_price'] = y_pred_rescaled\n",
    "\n",
    "cumulative_returns(df_lstm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Forecasting with Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "df_prophet = df.copy()\n",
    "df_prophet= df_prophet.dropna()\n",
    "df_prophet.sort_values(by=['date'], inplace=True)\n",
    "\n",
    "#Remove extreme outliers based on the percent change\n",
    "#remove changes greater than 3 standard deviations from the mean\n",
    "mean_change = df_prophet['percent_change_next_weeks_price'].mean()\n",
    "std_change = df_prophet['percent_change_next_weeks_price'].std()\n",
    "\n",
    "df_prophet = df_prophet[(df_prophet['percent_change_next_weeks_price'] > mean_change - 3 * std_change) &\n",
    "            (df_prophet['percent_change_next_weeks_price'] < mean_change + 3 * std_change)]\n",
    "\n",
    "#Split the dataset into training and testing based on date\n",
    "split = pd.to_datetime('2011-05-21')\n",
    "train_data = df_prophet[df_prophet.date < split]\n",
    "test_data = df_prophet[df_prophet.date >= split]\n",
    "\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "#Loop over each stock to fit a Prophet model and make predictions\n",
    "for stock in df_prophet['stock'].unique():\n",
    "    #print(f\"Running Prophet model for stock: {stock}\")\n",
    "    \n",
    "    #Filter data for the current stock\n",
    "    stock_train = train_data[train_data['stock'] == stock]\n",
    "    stock_test = test_data[test_data['stock'] == stock]\n",
    "    \n",
    "    \n",
    "    #Prepare data for Prophet\n",
    "    prophet_data = stock_train[['date', 'percent_change_next_weeks_price','close', 'volatility', 'rolling_avg_3w','lag_1']].copy()\n",
    "    #Prophet requires 'ds' for date and 'y' for target\n",
    "    prophet_data.columns = ['ds', 'y', 'close', 'volatility', 'rolling_avg_3w','lag_1']\n",
    "    \n",
    "    #Initialize and fit Prophet model with aditional regressors\n",
    "    model = Prophet(weekly_seasonality=True)\n",
    "    model.add_regressor('close')\n",
    "    model.add_regressor('volatility')\n",
    "    model.add_regressor('rolling_avg_3w')\n",
    "    model.add_regressor('lag_1')\n",
    "    model.fit(prophet_data)\n",
    "    \n",
    "    # Create future dataframe to predict for test set\n",
    "    future = pd.DataFrame(stock_test[['date', 'close', 'volatility', 'rolling_avg_3w','lag_1']])\n",
    "    future.columns = ['ds', 'close', 'volatility', 'rolling_avg_3w','lag_1']\n",
    "    #Make predictions\n",
    "    forecast = model.predict(future)\n",
    "   \n",
    "    #Store predictions with the stock information\n",
    "    stock_test['predicted_change_next_weeks_price'] = forecast['yhat_lower'].values\n",
    "    predictions = pd.concat([predictions, stock_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot actual vs predicted values for each stock in the test set\n",
    "for stock in df_prophet['stock'].unique():\n",
    "    stock_predictions = predictions[predictions['stock'] == stock]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(stock_predictions['date'], stock_predictions['percent_change_next_weeks_price'], label='Actual Percent Change', color='blue')\n",
    "    plt.plot(stock_predictions['date'], stock_predictions['predicted_change_next_weeks_price'], label='Predicted Percent Change', color='red')\n",
    "    plt.title(f'Stock: {stock} - Actual vs Predicted Percent Change')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Percent Change Next Week\\'s Price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(stock_predictions['percent_change_next_weeks_price'], stock_predictions['predicted_change_next_weeks_price'])\n",
    "print(f\"Mean Squared Error for stocks: {mse}\")\n",
    "\n",
    "rmse = root_mean_squared_error(stock_predictions['percent_change_next_weeks_price'], stock_predictions['predicted_change_next_weeks_price'])\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sort_values(by=['date'], inplace=True)\n",
    "cumulative_returns(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "*Add here your modelling results and conclusions (max. 200 words)*\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In the conclusions, you should enumerate the results you got after completing the challenge.\n",
    "* How good do you consider your results? \n",
    "* What are some factors that would contribute to get better results?\n",
    "* What are some advantages and disadvantages of your solution?\n",
    "* What can be done as future work to improve your results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "*Add here your final conclusions (max. 400 words)*\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "*Add here your thoughts and feedback regarding this challenge.*\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit your solution you should e-mail us this notebook in response to the e-mail you initially received with the challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
